{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3062\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3063\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3064\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c6efcf1c9f6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreatDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0mfeatLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0mmyTree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatLabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m     \u001b[0mmaxdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetTreeDepth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'maxdepth:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-c6efcf1c9f6c>\u001b[0m in \u001b[0;36mcreateTree\u001b[0;34m(dataset, labels, featLabels)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mclassList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#若类别完全相同则停止划分\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mclassList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmostCnt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mbestFeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselectBestFeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#选择最优的特征\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2683\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2685\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2690\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2692\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2484\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2486\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2487\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4115\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4116\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3063\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3064\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3065\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib.pyplot as plt\n",
    "from math import log\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "import pickle\n",
    "\n",
    "'''计算经验熵,反应随机变量的不确定性'''\n",
    "def calEnt(dataset): \n",
    "    numEntries = len(dataset)\n",
    "    labelCounts = {}\n",
    "    for featVec in dataset:  #为所有可能的类别标签创建字典\n",
    "        currentLabel = featVec[-1]\n",
    "        if currentLabel not in labelCounts.keys():   #将没有出现的标签放进字典统计\n",
    "            labelCounts[currentLabel] = 0\n",
    "        labelCounts[currentLabel] += 1\n",
    "    Ent = 0.0  # 经验熵\n",
    "    for key in labelCounts:\n",
    "        prob = float(labelCounts[key]) / numEntries  #该标签出现的概率\n",
    "        Ent -= prob * log(prob,2)  \n",
    "    return Ent\n",
    "\n",
    "'''创建数据集'''\n",
    "def creatDataSet():\n",
    "#     dataSet = [ [0, 0, 0, 0, 'no'], \n",
    "#                 [0, 0, 0, 1, 'no'],\n",
    "#                 [0, 1, 0, 1, 'yes'],\n",
    "#                 [0, 1, 1, 0, 'yes'],\n",
    "#                 [0, 0, 0, 0, 'no'],\n",
    "#                 [1, 0, 0, 0, 'no'],\n",
    "#                 [1, 0, 0, 1, 'no'],\n",
    "#                 [1, 1, 1, 1, 'yes'],\n",
    "#                 [1, 0, 1, 2, 'yes'],\n",
    "#                 [1, 0, 1, 2, 'yes'],\n",
    "#                 [2, 0, 1, 2, 'yes'],\n",
    "#                 [2, 0, 1, 1, 'yes'],\n",
    "#                 [2, 1, 0, 1, 'yes'],\n",
    "#                 [2, 1, 0, 2, 'yes'],\n",
    "#                 [2, 0, 0, 0, 'no']]\n",
    "#     labels = ['年龄', '是否有工作', '是否有自己的房子', '信贷情况']  #特征标签\n",
    "    dataSet = pd.read_csv('/Users/fanjiakuan/JupyterProject/Comp/TitanicProject/data/train_x1.csv')\n",
    "    labels = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'Title', 'IsAlone', 'Age*Pclass']\n",
    "    return dataSet, labels\n",
    "\n",
    "'''依据某个特征对数据进行分类'''\n",
    "def splitDataSet(dataset, axis, value):  #axis-划分数据集的特征,value-需要返回特征的值\n",
    "    retDataSet = []  #返回的数据列表\n",
    "    for featVec in dataset:\n",
    "        if featVec[axis] == value:\n",
    "            reducedFeatVec = featVec[:axis]\n",
    "            reducedFeatVec.extend(featVec[axis+1:]) #将符合条件的特征添加到待返回数据集当中\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "    return retDataSet\n",
    "\n",
    "'''依据信息增益准则选择最优特征'''\n",
    "def selectBestFeature(dataset):\n",
    "    numFeatures = len(dataset[0]) - 1  #特征数量\n",
    "    baseEnt = calEnt(dataset)  #经验熵\n",
    "    bestInfoGain = 0.0  #信息增益\n",
    "    bestFeatureIndex = -1  #最优特征索引值\n",
    "    dataset_len = float(len(dataset))  #数据长度\n",
    "    for i in range(numFeatures):\n",
    "        featList = [example[i] for example in dataset]  #获取数据集第i个特征所有值,但是会有大量重复值,需要去重\n",
    "        uniqueFeat = set(featList) \n",
    "        conEnt = 0.0  #经验条件熵\n",
    "        for value in uniqueFeat:  #计算信息增益\n",
    "            subDataSet = splitDataSet(dataset, i, value)\n",
    "            prob = len(subDataSet) / dataset_len  #计算子集的概率\n",
    "            conEnt += prob * calEnt(subDataSet)  #经验条件熵\n",
    "            infoGain = baseEnt - conEnt  #信息增益=熵-条件熵\n",
    "            #更新信息增益最大值\n",
    "            if(infoGain>bestInfoGain):\n",
    "                bestInfoGain = infoGain\n",
    "                bestFeatureIndex = i\n",
    "    return bestFeatureIndex\n",
    "    \n",
    "\n",
    "'''统计classList中出现最多元素的标签'''\n",
    "def mostCnt(classList):\n",
    "    classCount = {}\n",
    "    for vote in classList:\n",
    "        if vote not in classCount.keys():\n",
    "            classCount[vote] = 0  #如果类别不在字典就添加进去\n",
    "        classCount[vote] += 1\n",
    "    sortedClassCount = sorted(classCount.items(), key = operator.itemgetter(1), reverse = True)  #字典值降许排序\n",
    "    return sortedClassCount[0][0]  #返回出现次数最多的元素\n",
    "\n",
    "\n",
    "'''创建决策树'''\n",
    "def createTree(dataset, labels, featLabels):\n",
    "    classList = [example[-1] for example in dataset]  #取出类别标签\n",
    "    if classList.count(classList[0]) == len(classList):  #若类别完全相同则停止划分\n",
    "        return classList[0]\n",
    "    if len(dataset[0]) == 1 or len(labels) == 0:\n",
    "        return mostCnt(classList)\n",
    "    bestFeat = selectBestFeature(dataset)  #选择最优的特征\n",
    "    bestFeatLabel = labels[bestFeat]  #最优特征标签\n",
    "    featLabels.append(bestFeatLabel)\n",
    "    myTree = {bestFeatLabel:{}}  #根据最优特征标签生成树,根节点\n",
    "    del(labels[bestFeat])  #删除使用过得类别标签\n",
    "    featValues = [example[bestFeat] for example in dataset]  #取出训练集中所有最优特征的属性值\n",
    "    uniqueValues = set(featValues)  #去重\n",
    "    for value in uniqueValues:\n",
    "        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataset, bestFeat, value), labels, featLabels)\n",
    "    return myTree\n",
    "\n",
    "'''获取决策树叶子节点数目'''\n",
    "def getNumLeafs(myTree):\n",
    "    numLeafs = 0\n",
    "    firstStr = next(iter(myTree))\n",
    "    secondDict = myTree[firstStr]\n",
    "    for key in secondDict.keys():\n",
    "        if type(secondDict[key]).__name__ == 'dict': #测试该节点是否为字典,如果不是字典,代表为叶子节点\n",
    "            numLeafs += getNumLeafs(secondDict[key])\n",
    "        else:\n",
    "            numLeafs += 1\n",
    "    return numLeafs\n",
    "\n",
    "'''获取决策树的层数'''\n",
    "def getTreeDepth(myTree):\n",
    "    maxDepth = 0\n",
    "    firstStr = next(iter(myTree))\n",
    "    secondDict = myTree[firstStr]\n",
    "    for key in secondDict.keys():\n",
    "        if type(secondDict[key]).__name__ == 'dict':\n",
    "            thisDepth = getTreeDepth(secondDict[key]) + 1\n",
    "        else:\n",
    "            thisDepth = 1\n",
    "        if thisDepth > maxDepth:  #更新最大层数\n",
    "            maxDepth = thisDepth\n",
    "    return maxDepth\n",
    "\n",
    "'''使用决策树分类'''\n",
    "def classify(inputTree, featLabels, testVec):\n",
    "    firstStr = next(iter(inputTree))\n",
    "    secondDict = inputTree[firstStr]\n",
    "    featIndex = featLabels.index(firstStr)\n",
    "    for key in secondDict.keys():\n",
    "        if testVec[featIndex] == key:\n",
    "            if type(secondDict[key]).__name__ == 'dict':\n",
    "                classLabel = classify(secondDict[key], featLabels, testVec)\n",
    "            else:\n",
    "                classLabel = secondDict[key]\n",
    "    return classLabel\n",
    "\n",
    "'''存储决策树'''\n",
    "def storeTree(inputTree, filename):\n",
    "    with open(filename, 'wb') as fw:\n",
    "        pickle.dump(inputTree, fw)\n",
    "        \n",
    "'''读取决策树'''\n",
    "def grabTree(filename):\n",
    "    fr = open(filename, 'rb')\n",
    "    return pickle.load(fr)   \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataset,labels = creatDataSet()\n",
    "    featLabels = []\n",
    "    myTree = createTree(dataset, labels, featLabels)\n",
    "    maxdepth = getTreeDepth(myTree)\n",
    "    print('maxdepth:', maxdepth)\n",
    "#     testVec = [0,0]\n",
    "    testVec = [3, 0, 2, 0, 2, 1, 1, 6]\n",
    "#     testVec = pd.read_csv('/Users/fanjiakuan/JupyterProject/Comp/TitanicProject/data/test_x1.csv')\n",
    "    result = classify(myTree, featLabels, testVec)\n",
    "    if result == '0':\n",
    "        print('死亡')\n",
    "    else:\n",
    "        print('生存')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(     3  0  1  0.1  0.2  1.1  0.3  3.1  0.4\n",
       " 0    1  1  2    3    1    3    0    2    1\n",
       " 1    3  1  1    1    0    2    1    3    1\n",
       " 2    1  1  2    3    0    3    0    2    1\n",
       " 3    3  0  2    1    0    1    1    6    0\n",
       " 4    3  0  0    1    2    1    1    0    0\n",
       " 5    1  0  3    3    0    1    1    3    0\n",
       " 6    3  0  0    2    0    0    0    0    0\n",
       " 7    3  1  1    1    0    3    0    3    1\n",
       " 8    2  1  0    2    1    3    0    0    1\n",
       " 9    3  1  0    2    0    2    0    0    1\n",
       " 10   1  1  3    2    0    2    1    3    1\n",
       " 11   3  0  1    1    0    1    1    3    0\n",
       " 12   3  0  2    3    0    1    0    6    0\n",
       " 13   3  1  0    0    0    2    1    0    0\n",
       " 14   2  1  3    2    0    3    1    6    1\n",
       " 15   3  0  0    2    2    0    0    0    0\n",
       " 16   2  0  0    1    0    1    1    0    1\n",
       " 17   3  1  1    2    0    3    0    3    0\n",
       " 18   3  1  0    0    1    3    1    0    1\n",
       " 19   2  0  2    2    0    1    1    4    0\n",
       " 20   2  0  2    1    0    1    1    4    1\n",
       " 21   3  1  0    1    2    2    1    0    1\n",
       " 22   1  0  1    3    0    1    1    1    1\n",
       " 23   3  1  0    2    0    2    0    0    0\n",
       " 24   3  1  2    3    0    3    0    6    1\n",
       " 25   3  0  0    0    1    1    1    0    0\n",
       " 26   1  0  1    3    0    1    0    1    0\n",
       " 27   3  1  0    0    2    2    1    0    1\n",
       " 28   3  0  0    0    0    1    1    0    0\n",
       " 29   1  0  2    2    1    4    1    2    0\n",
       " ..  .. .. ..  ...  ...  ...  ...  ...  ...\n",
       " 860  2  0  1    1    0    1    0    2    0\n",
       " 861  1  1  2    2    0    3    1    2    1\n",
       " 862  3  1  0    3    0    2    0    0    0\n",
       " 863  2  0  1    1    0    1    1    2    0\n",
       " 864  2  1  2    1    0    3    1    4    1\n",
       " 865  2  1  1    1    1    2    0    2    1\n",
       " 866  1  0  1    3    0    1    1    1    0\n",
       " 867  3  0  0    1    0    1    1    0    0\n",
       " 868  3  0  0    1    0    0    0    0    1\n",
       " 869  3  0  1    0    0    1    1    3    0\n",
       " 870  1  1  2    3    0    3    0    2    1\n",
       " 871  1  0  2    0    0    1    1    2    0\n",
       " 872  3  0  2    1    0    1    1    6    0\n",
       " 873  2  1  1    2    1    3    0    2    1\n",
       " 874  3  1  0    0    1    2    1    0    1\n",
       " 875  3  0  1    1    0    1    1    3    0\n",
       " 876  3  0  1    0    0    1    1    3    0\n",
       " 877  3  0  0    0    0    1    1    0    0\n",
       " 878  1  1  3    3    1    3    0    3    1\n",
       " 879  2  1  1    2    0    3    0    2    1\n",
       " 880  3  0  2    0    0    1    1    6    0\n",
       " 881  3  1  1    1    0    2    1    3    0\n",
       " 882  2  0  1    1    0    1    1    2    0\n",
       " 883  3  0  1    0    0    1    1    3    0\n",
       " 884  3  1  2    2    2    3    0    6    0\n",
       " 885  2  0  1    1    0    4    1    2    0\n",
       " 886  1  1  1    2    0    2    1    1    1\n",
       " 887  3  1  0    2    0    2    0    0    0\n",
       " 888  1  0  1    2    1    1    1    1    1\n",
       " 889  3  0  1    0    2    1    1    3    0\n",
       " \n",
       " [890 rows x 9 columns],\n",
       " ['Pclass',\n",
       "  'Sex',\n",
       "  'Age',\n",
       "  'Fare',\n",
       "  'Embarked',\n",
       "  'Title',\n",
       "  'IsAlone',\n",
       "  'Age*Pclass'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def creatDataSet():\n",
    "#     dataSet = [ [0, 0, 0, 0, 'no'], \n",
    "#                 [0, 0, 0, 1, 'no'],\n",
    "#                 [0, 1, 0, 1, 'yes'],\n",
    "#                 [0, 1, 1, 0, 'yes'],\n",
    "#                 [0, 0, 0, 0, 'no'],\n",
    "#                 [1, 0, 0, 0, 'no'],\n",
    "#                 [1, 0, 0, 1, 'no'],\n",
    "#                 [1, 1, 1, 1, 'yes'],\n",
    "#                 [1, 0, 1, 2, 'yes'],\n",
    "#                 [1, 0, 1, 2, 'yes'],\n",
    "#                 [2, 0, 1, 2, 'yes'],\n",
    "#                 [2, 0, 1, 1, 'yes'],\n",
    "#                 [2, 1, 0, 1, 'yes'],\n",
    "#                 [2, 1, 0, 2, 'yes'],\n",
    "#                 [2, 0, 0, 0, 'no']]\n",
    "#     labels = ['年龄', '是否有工作', '是否有自己的房子', '信贷情况']  #特征标签\n",
    "    dataSet = pd.read_csv('/Users/fanjiakuan/JupyterProject/Comp/TitanicProject/data/train_x1.csv')\n",
    "    labels = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'Title', 'IsAlone', 'Age*Pclass']\n",
    "    return dataSet, labels\n",
    "\n",
    "creatDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
